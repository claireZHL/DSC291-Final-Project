{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Libraries\n",
    "\n",
    "import pandas as pd  # Start SEC 1\n",
    "import networkx as nx  # Start SEC 2\n",
    "from networkx.algorithms import bipartite  # Start SEC 2\n",
    "import community as community_louvain  # Start SEC 4\n",
    "import random, copy  # Start SEC 5\n",
    "from pyvis.network import Network  # Start SEC 6"
   ],
   "id": "cbec7141048818c6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## DSC291_Final_Project.ipynb\n",
    "# GitHub Developer Collaboration Network Analysis\n",
    "# -------------------------------------------------------------\n",
    "# This notebook loads your CSV data from the old notebook, preprocesses it into a clean\n",
    "# pivot table (contributors vs. repos), constructs a bipartite graph and then projects it\n",
    "# into a contributor network, performs network analysis (centrality, community detection,\n",
    "# robustness simulation), and exports visualizations for Gephi and Pyvis.\n",
    "#\n",
    "# NOTE: Ensure the CSV file (\"top_100_stars_nodes_df.csv\") is in your working directory.\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "## SEC 1. Data Loading & Preprocessing\n",
    "# Load the CSV file and clean it.\n",
    "\n",
    "csv_file = \"top_100_stars_nodes_df.csv\"\n",
    "df_raw = pd.read_csv(csv_file)\n",
    "print(\"Raw Data:\")\n",
    "print(df_raw.head())\n",
    "\n",
    "# If the CSV is in wide format with a 'contributors' column, set it as index and drop \"Unnamed: 0\"\n",
    "if 'contributors' in df_raw.columns:\n",
    "    df_clean = df_raw.set_index('contributors')\n",
    "    if 'Unnamed: 0' in df_clean.columns:\n",
    "        df_clean.drop(columns=['Unnamed: 0'], inplace=True)\n",
    "    df_clean = df_clean.fillna(0)\n",
    "    print(\"Data assumed to be in pivoted wide format:\")\n",
    "else:\n",
    "    # Otherwise, pivot the data (assume columns: contributor, repo, contributions)\n",
    "    df_clean = df_raw.pivot_table(index='contributor',\n",
    "                                  columns='repo',\n",
    "                                  values='contributions',\n",
    "                                  fill_value=0)\n",
    "    print(\"Pivoted Data (long format to wide):\")\n",
    "\n",
    "print(df_clean.head())\n",
    "\n",
    "# Ensure the index is unique by grouping duplicates (summing their values)\n",
    "if not df_clean.index.is_unique:\n",
    "    df_clean = df_clean.groupby(df_clean.index).sum()\n",
    "\n",
    "print(\"Is the index unique now?\", df_clean.index.is_unique)\n",
    "print(df_clean.head())\n",
    "df_clean.to_csv(\"top_100_stars_nodes_clean.csv\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## SEC 2. Building the Bipartite Graph and Contributor Projection\n",
    "# Create a bipartite graph with two sets of nodes: contributors and repositories.\n",
    "# Then project the graph onto the contributor nodes so that an edge exists between two\n",
    "# developers if they share at least one repository. The edge weight is the number of shared repos.\n",
    "\n",
    "B = nx.Graph()\n",
    "\n",
    "# Get lists of contributors and repos\n",
    "contributors = list(df_clean.index)\n",
    "repos = list(df_clean.columns)\n",
    "\n",
    "# Add contributor nodes (with attribute 'bipartite' = 'contributors')\n",
    "B.add_nodes_from(contributors, bipartite='contributors')\n",
    "# Add repository nodes (with attribute 'bipartite' = 'repos')\n",
    "B.add_nodes_from(repos, bipartite='repos')\n",
    "\n",
    "# Add edges: for each contributor, add an edge to a repo if the contribution count > 0.\n",
    "for contributor in contributors:\n",
    "    for repo in repos:\n",
    "        # Use .at to ensure a scalar value and convert it to float\n",
    "        contrib_value = df_clean.at[contributor, repo]\n",
    "        contrib_count = float(contrib_value)\n",
    "        if contrib_count > 0:\n",
    "            B.add_edge(contributor, repo, weight=contrib_count)\n",
    "\n",
    "print(\"Bipartite graph created:\")\n",
    "print(\"Number of contributor nodes:\", len(contributors))\n",
    "print(\"Number of repo nodes:\", len(repos))\n",
    "print(\"Total edges in bipartite graph:\", B.number_of_edges())\n",
    "\n",
    "# Project the bipartite graph onto contributor nodes.\n",
    "G = bipartite.weighted_projected_graph(B, contributors)\n",
    "print(\"Contributor collaboration network:\")\n",
    "print(\"Nodes:\", G.number_of_nodes(), \"Edges:\", G.number_of_edges())"
   ],
   "id": "aeccedf4b627965e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## SEC 3. Network Analysis: Centrality Measures\n",
    "# Compute various centrality measures (degree, betweenness, closeness, eigenvector)\n",
    "# and assign these values as node attributes.\n",
    "\n",
    "deg_centrality = nx.degree_centrality(G)\n",
    "betw_centrality = nx.betweenness_centrality(G, normalized=True)\n",
    "closeness_centrality = nx.closeness_centrality(G)\n",
    "eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)\n",
    "\n",
    "for node in G.nodes():\n",
    "    G.nodes[node]['deg_cent'] = deg_centrality[node]\n",
    "    G.nodes[node]['betw_cent'] = betw_centrality[node]\n",
    "    G.nodes[node]['clos_cent'] = closeness_centrality[node]\n",
    "    G.nodes[node]['eig_cent'] = eigenvector_centrality[node]\n",
    "\n",
    "top_deg = sorted(deg_centrality.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "print(\"Top 5 by degree centrality:\")\n",
    "for node, score in top_deg:\n",
    "    print(f\"  {node}: {score:.4f}\")"
   ],
   "id": "95d0295ab9a33b6a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## SEC 4. Community Detection Using the Louvain Method\n",
    "# Use the python-louvain package to detect communities and add the community\n",
    "# information as a node attribute.\n",
    "\n",
    "partition = community_louvain.best_partition(G)\n",
    "for node, comm in partition.items():\n",
    "    G.nodes[node]['community'] = comm\n",
    "\n",
    "num_communities = len(set(partition.values()))\n",
    "print(f\"Detected {num_communities} communities using Louvain.\")"
   ],
   "id": "60109dbaa6dd4250"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## SEC 5. Network Robustness Simulation\n",
    "# Simulate the impact on the largest connected component (LCC) of the network by\n",
    "# removing nodes both by targeted (highest degree first) and at random.\n",
    "\n",
    "import random, copy\n",
    "\n",
    "def largest_component_size(graph):\n",
    "    if graph.number_of_nodes() == 0:\n",
    "        return 0\n",
    "    return len(max(nx.connected_components(graph), key=len))\n",
    "\n",
    "G_targeted = copy.deepcopy(G)\n",
    "G_random = copy.deepcopy(G)\n",
    "\n",
    "targeted_nodes = sorted(G_targeted.degree(), key=lambda x: x[1], reverse=True)\n",
    "component_sizes_targeted = []\n",
    "\n",
    "print(\"Starting targeted removal simulation:\")\n",
    "for i, (node, _) in enumerate(targeted_nodes):\n",
    "    G_targeted.remove_node(node)\n",
    "    if (i + 1) % 100 == 0:\n",
    "        size = largest_component_size(G_targeted)\n",
    "        component_sizes_targeted.append(size)\n",
    "        print(f\"  Removed {i+1} nodes, largest component size: {size}\")\n",
    "\n",
    "nodes_random = list(G_random.nodes())\n",
    "random.shuffle(nodes_random)\n",
    "component_sizes_random = []\n",
    "\n",
    "print(\"Starting random removal simulation:\")\n",
    "for i, node in enumerate(nodes_random):\n",
    "    G_random.remove_node(node)\n",
    "    if (i + 1) % 100 == 0:\n",
    "        size = largest_component_size(G_random)\n",
    "        component_sizes_random.append(size)\n",
    "        print(f\"  Removed {i+1} nodes, largest component size: {size}\")"
   ],
   "id": "bd45a84bebad7bbf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "## SEC 6. Visualization Export\n",
    "# Export the network for external visualization in G (GEXF format)\n",
    "# and create an interactive visualization using Pyvis.\n",
    "\n",
    "gephi_filename = \"github_collaboration.gexf\"\n",
    "nx.write_gexf(G, gephi_filename)\n",
    "print(f\"Graph exported to {gephi_filename} for Gephi visualization.\")\n",
    "\n",
    "from pyvis.network import Network\n",
    "\n",
    "net = Network(height=\"750px\", width=\"100%\", notebook=True)\n",
    "net.from_nx(G)\n",
    "\n",
    "for node in net.nodes:\n",
    "    comm = G.nodes[node['id']].get('community', 0)\n",
    "    node['color'] = 'red' if comm == 0 else 'blue'\n",
    "\n",
    "pyvis_filename = \"github_collaboration.html\"\n",
    "net.show(pyvis_filename)\n",
    "print(f\"Interactive network saved as {pyvis_filename}\")"
   ],
   "id": "45ec33bda4b2fad6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "##  Outline of this notebook:\n",
    "# - Loads and preprocesses the CSV data.\n",
    "# - Aggregates duplicate contributors.\n",
    "# - Constructs a bipartite graph and projects it into a contributor network.\n",
    "# - Computes centrality measures.\n",
    "# - Detects communities with the Louvain method.\n",
    "# - Simulates network robustness via node removal.\n",
    "# - Exports visualizations for Gephi and Pyvis.\n",
    "#"
   ],
   "id": "988d0d6a196bf8fa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
